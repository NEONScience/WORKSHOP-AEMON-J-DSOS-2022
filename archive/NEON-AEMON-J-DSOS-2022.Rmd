---
syncID: 317ecab8e00b4a959a76dba181bb33b8
title: "Download and work with NEON Aquatic Instrument Data"
description: "Tutorial for downloading NEON AIS data using the neonUtilities package and then exploring and understanding the downloaded data"
dateCreated: 2021-04-05
authors: Bobby Hensley, Guy Litt, Megan Jones, Nora Catolico, Kelly Hondula
contributors: Kaelin M. Cawley
estimatedTime: 2 hours
packagesLibraries: neonUtilities, ggplot2, lubridate, plyr
topics: data-management, rep-sci
languageTool: R, API
code1: R/download-ais-data/download-NEON-AIS-data.R
tutorialSeries:
urlTitle: explore-neon-ais-data
editor_options: 
  chunk_output_type: inline
---

## Introduction Links

Getting started with NEON data: https://www.neonscience.org/resources/getting-started-neon-data-resources

Contact us form: https://www.neonscience.org/about/contact-us

Teaching Modules: https://www.neonscience.org/resources/learning-hub/teaching-modules <br />
QUBES modules: https://qubeshub.org/community/groups/neon/educational_resources <br />
EDDIE modules : https://serc.carleton.edu/eddie/macrosystems/index.html

Spatial data and maps: https://neon.maps.arcgis.com/home/index.html

NEON data portal: https://data.neonscience.org/

NEONScience GitHub repo: https://github.com/NEONScience <br />
JASM 2022 AIS Workshop GitHub repo: https://github.com/NEONScience/WORKSHOP-JASM-2022-AIS

## Tutorial

This tutorial covers downloading NEON Aquatic Instrument System (AIS) data 
using the neonUtilities R package, as well as basic instruction in beginning to 
explore and work with the downloaded data.  This includes navigating data
documentation, separating data using the horizontal location (HOR) variable, 
interpreting quality flags, and combining multiple data sets. 

<div id="ds-objectives" markdown="1"


## Objectives

After completing this activity, you will be able to:

* Download NEON AIS data using the `neonUtilities` package.
* Understand downloaded data sets and load them into R for analyses.
* Separate data collected at different sensor locations using the HOR variable.
* Understand and interpret quality flags, including how to discover what non-
* standard quality flags mean.
* Combine datasets using shared identifiers (in this case time stamps).

## Things You'll Need To Complete This Tutorial
To complete this tutorial you will need R (version >3.4) and, 
preferably, RStudio loaded on your computer.

### Install R Packages

* **neonUtilities**: Basic functions for accessing NEON data
* **ggplot2**: Plotting functions
* **lubridate**: Date manipulation functions
* **plyr**: Data manipulation functions

These packages are on CRAN and can be installed by 
`install.packages()`.

### Additional Resources

* <a href="https://github.com/NEONScience/NEON-Utilities/neonUtilities" target="_blank">GitHub repository for neonUtilities</a>

</div>

## Download Files and Load Directly to R: loadByProduct()

The most popular function in `neonUtilities` is `loadByProduct()`. 
This function downloads data from the NEON API, merges the site-by-month 
files, and loads the resulting data tables into the R environment, 
assigning each data type to the appropriate R class. This is a popular 
choice because it ensures you're always working with the most up-to-date data, 
and it ends with ready-to-use tables in R. However, if you use it in
a workflow you run repeatedly, keep in mind it will re-download the 
data every time.

Before we get the NEON data, we need to install (if not already done) and load 
the neonUtilities R package, as well as other packages we will use in the 
analysis. 

```{r set-up-env, eval=F}
# # Install neonUtilities package if you have not yet.
# install.packages("neonUtilities")
# install.packages("ggplot2")
# install.packages("lubridate")
# install.packages("plyr")
```

```{r load-packages}
# Set global option to NOT convert all character variables to factors
options(stringsAsFactors=F)

# Load required packages
library(neonUtilities)
library(ggplot2)
library(lubridate)
library(plyr)
```

The inputs to `loadByProduct()` control which data to download and how 
to manage the processing. The following are frequently used inputs: 

* `dpID`: the data product ID, e.g. DP1.20288.001
* `site`: defaults to "all", meaning all sites with available data; 
can be a vector of 4-letter NEON site codes, e.g. 
`c("MART","ARIK","BARC")`.
* `startdate` and `enddate`: defaults to NA, meaning all dates 
with available data; or a date in the form YYYY-MM, e.g. 
2017-06. Since NEON data are provided in month packages, finer 
scale querying is not available. Both start and end date are 
inclusive.
* `package`: either basic or expanded data package. Expanded data 
packages generally include additional information about data 
quality, such as individual quality flag test results. Not every 
NEON data product has an expanded package; if the expanded package 
is requested but there isn't one, the basic package will be 
downloaded.
* `release`: The data release to be downloaded; either 'current' 
or the name of a release, e.g. 'RELEASE-2021'. 'current' returns 
provisional data in addition to the most recent release. To 
download only provisional data, use release='PROVISIONAL'. 
Defaults to 'current'. See 
https://www.neonscience.org/data-samples/data-management/data-revisions-releases 
for more information
* `avg`: defaults to "all", to download all data; or the 
number of minutes in the averaging interval. See example below; 
only applicable to IS data.
* `check.size`: T or F; should the function pause before downloading 
data and warn you about the size of your download? Defaults to T; if 
you are using this function within a script or batch process you 
will want to set this to F.
* `token`: this allows you to input your NEON API token to obtain faster 
downloads. 
Learn more about NEON API tokens in the <a href="https//:www.neonscience.org/neon-api-tokens-tutorial" target="_blank">**Using an API Token when Accessing NEON Data with neonUtilities** tutorial</a>. 

There are additional inputs you can learn about in the 
<a href="https//:www.neonscience.org/neonDataStackR" target="_blank">**Use the neonUtilities R Package to Access NEON Data** tutorial</a>. 

The `dpID` is the data product identifier of the data you want to 
download. The DPID can be found on the 
<a href="http://data.neonscience.org/data-products/explore" target="_blank">
Explore Data Products page</a>.

It will be in the form DP#.#####.###. For this tutorial, we'll use some data
products collected in NEON's Aquatic Instrument System: 

* DP1.20288.001: Water quality
* DP1.20033.001: Nitrate in surface water
* DP4.00130.001: Continuous discharge

Now it's time to consider the NEON field site of interest. If not specified, 
the default will download a data product from all sites. The following are 
4-letter site codes for NEON's 34 aquatics sites as of 2022:

ARIK = Arikaree River CO        
BARC = Barco Lake FL          
BIGC = Upper Big Creek CA       
BLDE = Black Deer Creek WY      
BLUE = Blue River OK            
BLWA = Black Warrior River AL 
CARI = Caribou Creek AK         
COMO = Como Creek CO          
CRAM = Crampton Lake WI         
CUPE = Rio Cupeyes PR           
FLNT = Flint River GA           
GUIL = Rio Guilarte PR 
HOPB = Lower Hop Brook MA       
KING = Kings Creek KS         
LECO = LeConte Creek TN         
LEWI = Lewis Run VA             
LIRO = Little Rock Lake WI      
MART = Martha Creek WA
MAYF = Mayfield Creek AL        
MCDI = McDiffett Creek KS    
MCRA = McRae Creek OR           
OKSR = Oksrukuyik Creek AK      
POSE = Posey Creek VA           
PRIN = Pringle Creek TX       
PRLA = Prairie Lake ND          
PRPO = Prairie Pothole ND     
REDB = Red Butte Creek UT       
SUGG = Suggs Lake FL            
SYCA = Sycamore Creek AZ        
TECR = Teakettle Creek CA        
TOMB = Lower Tombigbee River AL  
TOOK = Toolik Lake AK         
WALK = Walker Branch TN         
WLOU = West St Louis Creek CO       

```{r into-vars, results='hide'}
siteID <- "COMO"
startDateIn <- "2022-04"
endDateIn <- "2022-04"
myPathToGraphics <- "~/GitHub/WORKSHOP-AEMON-J-DSOS-2022/"

```

In this exercise, we will pull data from only one NEON field site for one month. Quick link to the `r siteID` site page: `r paste0("https://www.neonscience.org/field-sites/", siteID)`. 
Just substitute the 4-letter site code for any other site at the end of the url. 

Now let us download our data. If you are not using a NEON token to download 
your data, neonUtilities will ignore the `token` input. We set `check.size = F`  
so that the script runs well but remember you always want to check your 
download size first.

```{r download-data-waq, results='hide'}

# download data of interest - Water Quality
waq <- neonUtilities::loadByProduct(dpID="DP1.20288.001", 
                     site=siteID, 
                     startdate=startDateIn, 
                     enddate=endDateIn, 
                     package="expanded",
                     release= "current",
                     token = Sys.getenv("NEON_TOKEN"),
                     check.size = F)

```



## Files Associated with Downloads

The data we've downloaded comes as an object that is a named list of objects. 
To work with each of them, select them from the list using the `$` operator. 

```{r names-waq}
# view all components of the list
names(waq)

```

We can see that there are four objects in the downloaded water quality data. One 
dataframe of data (`waq_instantaneous`) and three metadata files. Now lets view
'waq_instantaneous'.

```{r view-data-waq}
# View the dataFrame
View(waq$waq_instantaneous)

```


### Saving out a file to work with in another program
```{r save-csv-waq}
# Save out a csv
utils::write.csv(waq$waq_instantaneous,
                 "~/GitHub/WORKSHOP-JASM-2022-AIS/waq_instantaneous.csv",
                 row.names = FALSE)

```

<div id="ds-challenge" markdown="1">
### Challenge: Download Other Related Data products
  
Using what you've learned above, can you modify the code to download data for 
the following parameters?

* DP1.20033.001: Nitrate in surface water
* DP4.00130.001: Continuous discharge
* The expanded data tables
* Dates matching the other data products you've downloaded

</div>

```{r download-data-nsw, results='hide'}
# download data of interest - Nitrate in Surface Water
nsw <-  loadByProduct(dpID="DP1.20033.001", site=siteID, 
                      startdate=startDateIn, enddate=endDateIn, 
                      package="expanded", 
                      token = Sys.getenv("NEON_TOKEN"),
                      check.size = F)

csd <- loadByProduct(dpID="DP4.00130.001", site=siteID,
                     startdate=startDateIn, enddate=endDateIn,
                     package="expanded",
                     token = Sys.getenv("NEON_TOKEN"),
                     check.size = F)

# # Also for pulling geomorphology survey
# geo <- loadByProduct(dpID="DP4.00131.001", site=siteID,
#                      package="expanded",
#                      token = Sys.getenv("NEON_TOKEN"),
#                      check.size = F)
# geoResults <- geo$geo_resultsFile
# geoResults$dataFilePath
# Paste url into a browser and the zip file with shapefiles and kmz and pdf will download

# # Or if you're getting a bunch you could also use neonUtilities function (uncomment to use) to access geomorphology data
# geoFolder <- zipsByProduct(dpID="DP4.00131.001")

```

If you'd like you can use the `$` operator to assign an object from an item in 
the list. If you prefer to extract each table from the list and work with it as 
independent objects, which we will do, you can use the `list2env()` function. 

```{r unlist-vars}
# unlist the variables and add to the global environment
list2env(waq, .GlobalEnv)
```

So what exactly are these four files and why would you want to use them? 

* **data file(s)**: There will always be one or more dataframes that include the 
primary data of the data product you downloaded. Multiple dataframes are available when there are related datatables for a single data product.
* **readme_xxxxx**: The readme file, with the corresponding 5 digits from the 
data product number, provides you with important information relevant to the 
data product and the specific instance of downloading the data. Here you can find manual flagging notes for all sites, locations, and time periods.
* **sensor_postions_xxxxx**: this file contains information about the coordinates
of each sensor, relative to a reference location. 
* **variables_xxxxx**: this file contains all the variables found in the 
associated data table(s). This includes full definitions, units, and other 
important information. 

Let's perform the same thing for the surface water nitrate and elevation of 
surface water data products too:
```{r unlist-remainder}
list2env(nsw, .GlobalEnv)
list2env(csd, .GlobalEnv)
```

```{r site-timezone-info}
# # Just a little snippet if anyone wants to convert to local time
# # NEON data downloads are all in UTC timezone, unless otherwise specified
# devtools::install_github(repo = "NEONScience/NEON-geolocation", 
#                          subdir = "geoNEON")
# 
# siteLocInfo <- geoNEON::getLocBySite(siteID, 
#                                      type = "site", 
#                                      history = F, 
#                                      token = NA_character_)
# 
# localTZ <- siteLocInfo$siteTimezone
# 
# waq_instantaneous$localStartTime <- format(waq_instantaneous$startDateTime,"%Y-%m-%dT%H:%M",tz = localTZ)

```

Note that a few more objects were added to the Global Environment, including:

* `NSW_15_minute`
* `csd_continuousDischarge`

The `15_minute` name indicates the time-averaging intervals in a dataset. Other 
examples may include `5_min` and `30_min` in the same data product, such as 
elevation of surface water (DP1.20016.001). If only one time average interests 
you, you may specify the time interval of interest when downloading the data when
calling `neonUtilities::loadByProduct()`.

## Data from Different Sensor Locations (HOR)

NEON often collects the same type of data from sensors in different locations. These 
data are delivered together but you will frequently want to plot the data 
separately or only include data from one sensor in your analysis. NEON uses the 
`horizontalPosition` variable in the data tables to describe which sensor 
data is collected from. The `horizontalPosition` is always a three digit number 
for AIS data. Non-shoreline HOR examples as of 2020 at AIS sites include:

* 101: stream sensors located at the **upstream** station on a **monopod mount**, 
* 111: stream sensors located at the **upstream** station on an **overhead cable mount**, 
* 131: stream sensors located at the **upstream** station on a **stand alone pressure transducer mount**, 
* 102: stream sensors located at the **downstream** station on a monopod mount, 
* 112: stream sensors located at the **downstream** station on an **overhead cable mount** 
* 132: stream sensors located at the **downstream** station on a **stand alone pressure transducer mount**, 
* 110: **pressure transducers** mounted to a **staff gauge**. 
* 103: sensors mounted on **buoys in lakes or rivers**
* 130 and 140: sensors mounted in the **littoral zone** of lakes

You'll frequently want to know which sensor locations are represented in your 
data. We can do this by looking for the `unique()` position designations in 
`horizontalPostions`. 

```{r waq-hor-num-locations}
# which sensor locations exist for water quality, DP1.20288.001?
cat("Water quality horizontal positions: ", unique(waq_instantaneous$horizontalPosition))

```

We can see that there are two water quality sensor positions at `r siteID` in `r startDateIn`. 
As the locations of sensors can change at sites over time (especially with 
aquatic sensors as AIS sites undergo redesigns) it is a good idea to check 
horizontal positions when you're adding in new locations or a new date range to 
your analyses. 

Let's check the HOR locations for surface water nitrate and continuous discharge too:
```{r all-hor-num-locations}
# which sensor locations exist for other data products?
print("Nitrate in Surface Water horizontal positions: ")
unique(NSW_15_minute$horizontalPosition)

print("Continuous Discharge horizontal positions: ")
unique(csd_continuousDischarge$stationHorizontalID)
```

Now we can use this information to split water quality data into the two
different sensor set locations: upstream and the downstream. 

```{r split-hor}
# Split data into separate dataframes by upstream/downstream locations.

waq_up <- 
  waq_instantaneous[(waq_instantaneous$horizontalPosition=="101"),]
waq_down <- 
  waq_instantaneous[(waq_instantaneous$horizontalPosition=="102"),]

```

## Plot Data

Now that we have our data separated into the upstream and downstream data, let's
plot both of the data sets together. We want to create a plot of the measures of
Dissolved Oxygen from the downstream sensor set (HOR = 102). 

First, let's identify the column names important for plotting - time and 
dissolved oxygen data:
```{r column-names}
# One option is to view column names in the data frame
colnames(waq_instantaneous)

# Alternatively, view the variables object corresponding to the data product for more information
View(variables_20288)
```
Quite a few columns in the water quality data product!

The time column we'll consider for instrumented systems is `endDateTime` 
because it approximately represents data within the interval on or before the 
`endDateTime` time stamp. Timestamp column choice matters for time-aggregated 
datasets, but should not matter for instantaneous data such as water quality.

When interpreting data, keep in mind NEON timestamps are always in UTC.

The data column we would like to plot is labeled `dissolvedOxygen`.

```{r plot-wqual}
# plot
plot_DO <- ggplot(data =waq_down,
                   aes(endDateTime, dissolvedOxygen)) +
                   geom_line(na.rm=TRUE, color="blue") + 
                    ylab("DO (mg/L)") + xlab(" ") +
                   ggtitle(paste0(siteID," Water Quality - DO"))

plot_DO

```


Now let's try plotting fDOM. fDOM is only measured at the downstream location
and is found in the same dataframe ('waq_down').  The column we would like to
plot is labeled 'fDOM'. (Note there is also a column labeled rawCalibratedfDOM.
This is raw data from the YSI multisonde which has not been absorbance
corrected).

```{r plot-fdom-ucert}
# plot
plot_fDOM <- ggplot(data =waq_down,
                   aes(endDateTime, fDOM)) +
                   geom_line(na.rm=TRUE, color="red") + 
                  ylab("fDOM QSU") + xlab(" ") +
                   ggtitle(paste0(siteID," Water Quality - fDOM"))

plot_fDOM

```

<div id="ds-challenge" markdown="1">
### Challenge: Plot Nitrate in Surface Water Data

Using what you've learned above, identify column names for nitrate in surface water 
(DP1.20033.001) and continuous discharge (DP4.00130.001)

</div>

```{r challenge-explore-nsw, results='hide'}
print("Nitrate in surface water column names:")
# what is the column name of the data stream of interest?
names(NSW_15_minute)

print("Continuous discharge column names:")
# what is the column name of the data stream of interest?
names(csd_continuousDischarge)

```

Using what you've learned above, plot nitrate in surface water.

```{r challenge-plot-nsw}
# plot
plot_NSW <- ggplot(data = NSW_15_minute,
                   aes(endDateTime, surfWaterNitrateMean)) +
                   geom_line(na.rm=TRUE, color="green") + 
                   ylab("NO3-N (uM)") + xlab(" ") +
                   ggtitle(paste0(siteID," Nitrate in Surface Water"))

plot_NSW

```


Note most NEON data products are published with uncertainty values.
Continuous discharge uses a Bayesian Model, with uncertainty in the
model priors (parametric uncertainty) and additional uncertainty from 
how well the posterior model fits the observations (remnant uncertainty).
Lets try adding the remnant uncertainty to the continuous discharge plot.

```{r challenge-plot-csd}
# plot
plot_Q <- ggplot() +
	geom_line(data = csd_continuousDischarge, 
	          aes(endDate, maxpostDischarge), 
	          na.rm=TRUE, color="black") +
  geom_ribbon(data=csd_continuousDischarge, 
              aes(x=endDate, 
                  ymin = (withRemnUncQLower1Std), 
                  ymax = (withRemnUncQUpper1Std)), 
              alpha = 0.4, fill = "grey75") +
	geom_line( na.rm = TRUE) +
	ylab("Q (L/s)") +
	xlab(" ") +
  ggtitle(paste0(siteID," Continuous Discharge"))

plot_Q

```


## Examine Quality Flagged Data

Data product quality flags fall under two distinct types:

* Automated quality flags, e.g. range, spike, step, null
* Manual science review quality flag

In instantaneous data such as water quality DP1.20288.001,
the quality flag columns are denoted with "QF".

In time-averaged data, most quality flags have been aggregated into
quality metrics, with column names denoted with "QM" representing
the fraction of flagged points within the time averaging window.

```{r view-qf}
waq_qf_names <- names(waq_down)[grep("QF", names(waq_down))]

print(paste0("Total columns in DP1.20288.001 expanded package = ", 
             as.character(length(waq_qf_names))))

# water quality has 96 data columns with QF in the name, 
# so let's just look at those corresponding to fDOM
print("fDOM columns in DP1.20288.001 expanded package:")
print(waq_qf_names[grep("fDOM", waq_qf_names)])

```

A quality flag (QF) of 0 indicates a pass, 1 indicates a fail, and -1 indicates
a test that could not be performed. For example, a range test cannot be 
performed on missing measurements.

Detailed quality flags test results are all available in the 
`package = 'expanded'` setting we specified when calling 
`neonUtilities::loadByProduct()`. If we had specified `package = 'basic'`,
we wouldn't be able to investigate the detail in the type of data flag thrown. 
We would only see the FinalQF columns.

The `AlphaQF` and `BetaQF` represent aggregated results of various QF tests, 
and vary by a data  product's algorithm. In most cases, an observation's 
`AlphaQF = 1` indicates whether or not at least one QF was set to a value 
of 1, and an observation's `BetaQF = 1` indicates whether or not at least one 
QF was set to value of -1.

Note that fDOM has a couple other data-stream specific QFs beyond the standard 
quality flags. These are specific to the algorithms used to correct raw fDOM 
readings using temperature and absorbance per Watras et al. (2011) and 
Downing et al. (2012).

Let's consider what types of fDOM quality flags were thrown.

```{r view-qf-fdom}
waq_qf_names <- names(waq_down)[grep("QF", names(waq_down))]

print(paste0("Total QF columns: ",length(waq_qf_names)))

# water quality has 96 data columns with QF in the name, 
# so let us just look at those corresponding to fDOM
fdom_qf_names <- waq_qf_names[grep("fDOM",waq_qf_names)]

for(col_nam in fdom_qf_names){
  print(paste0(col_nam, " unique values: ", 
               paste0(unique(waq_down[,col_nam]), 
                      collapse = ", ")))
}

```

Now let's consider the total number of flags generated for each quality test:

```{r dig-into-qf, echo=TRUE}
# Loop across the fDOM QF column names. 
#  Within each column, count the number of rows that equal '1'.
print("FLAG TEST - COUNT")
for (col_nam in fdom_qf_names){
  totl_qf_in_col <- length(which(waq_down[,col_nam] == 1))
  print(paste0(col_nam,": ",totl_qf_in_col))
}

# Let's also check out how many fDOMAbsQF = 2 exist
print(paste0("fDOMAbsQF = 2: ",
             length(which(waq_down[,"fDOMAbsQF"] == 2))))

print(paste0("Total fDOM observations: ", nrow(waq_down) ))
```

Above lists the total fDOM QFs from `r startDateIn` at `r siteID`, as well as the
total number of observation data points in the data file.

For specific details on the algorithms used to create a data product and its 
corresponding quality tests, it's best to first check the data product's 
Algorithm Theoretical Basis Document (ATBD). For water quality, that is 
NEON.DOC.004931 listed as Documentation references in the README file and the 
data product's web page.

Are there any manual science review quality flags? If so, the explanation for 
flagging may also be viewed in the data product's README file or in the data 
product's web page on NEON's data portal.

## Filtering (Some) Quality Flagged Observations

A simple approach to removing quality flagged observations is to remove data 
when the finalQF is raised. Let's view fDOM in the context of its final
quality flags:
```{r finalqf-context-fdom}
# Map QF label names for the plot for the fDOMFinalQF grouping
group_labels <- c("fDOMFinalQF = 0", "fDOMFinalQF = 1")
names(group_labels) <- c("0","1")

# Plot fDOM data, grouping by the fDOMFinalQF value
ggplot2::ggplot(data = waq_down, 
                aes(x = endDateTime, y = fDOM, group = fDOMFinalQF)) +
  ggplot2::geom_step() +
  facet_grid(fDOMFinalQF ~ ., 
             labeller = labeller(fDOMFinalQF = group_labels)) +
  ggplot2::ggtitle(paste0(siteID," Sensor Set 102 fDOM final QF comparison"))

```

The top panel corresponding to `fDOMFinalQF = 0` represents all fDOM data 
that were not flagged.  Conversely, the `fDOMFinalQF = 1` represents all
flagged fDOM data. Clearly, many data points that should have been auto
flagged were (example NAs).  But also, a few data points that perhaps
shouldn't have been were (example steps during the rising limb of storms).
Also note that the automated quality flag algorithms are not perfect, and 
suspect data points may occasionally pass the quality tests.

Rather than  relying on the `FinalQF` column to omit any quality flags, 
we encourage users to  download the expanded data package (which contains 
individual QF) and determine for themselves which data points to include or 
omit.

## Data Aggregation

Sensor data users commonly wish to aggregate data across two different 
datasets. In the following example, we will show how to combine fDOM from 
the water quality (DP1.20288.001) data product and continuous discharge 
(DP4.00130.001) into a single dataframe.

```{r subset-fdom}
# First, we want to subset all of the data related to fDOM.  We can do this
# using a grep function to target all the column names containing "fDOM".
# The timestamp (which we need to aggregate) is shared by all data in
# water quality and does not have "fDOM" in the name, so we must first add it.
names(waq_down)[names(waq_down) == "endDateTime"] <- "fDOMendDate"
fDOM <- waq_down[,grep("fDOM",names(waq_down))]

# We may also want to remove any quality flagged measurements.
fDOM<-fDOM[(fDOM$fDOMFinalQF==0),]
# And lets do the same thing for continuous discharge.
csd_continuousDischarge<-csd_continuousDischarge[(csd_continuousDischarge$dischargeFinalQF==0),]

```

Next we need to ensure that the time stamps will match correctly.

```{r check-POSIXct}
# Let's check to make sure our time columns are in POSIXct format, which is 
# needed if you download and read-in NEON data files without using the 
# neonUtilities package.
if("POSIXct" %in% class(fDOM$fDOMendDate)){
  print("Time column in fDOM is appropriately in POSIXct format")
} else {
  print("Converting fDOM endDate column to POSIXct")
  fDOM$fDOMendDate <- as.POSIXct(fDOM$fDOMendDate, tz = "UTC")
}

if("POSIXct" %in% class(csd_continuousDischarge$endDate)){
  print("Time column in csd_continuousDischarge is appropriately in POSIXct format")
} else {
  print("Converting csd_continuousDischarge column to POSIXct")
  csd_continuousDischarge$endDate <- as.POSIXct(csd_continuousDischarge$endDate, tz = "UTC")
}

#  Water quality and continuous discharge are both reported in 1 minute
# intervals.  But we still need to ensure that the time stamps will be an 
# exact match by rounding them to the nearest minute. We can do this using
# the lubridate package. 
fDOM$fDOMendDate<-lubridate::round_date(fDOM$fDOMendDate,unit="1 minute")
csd_continuousDischarge$endDate<-lubridate::round_date(csd_continuousDischarge$endDate,unit="1 minute")

```

Now we can merge the dataframes.

```{r merge-fDOM-csd}
# In this case we are only interested in paired measurements. To retain all 
# measurements even if there is not a corresponding measurement in the other 
# dataframe, change all.x & all.y to TRUE.
mergedData<-merge(fDOM,csd_continuousDischarge,by.x="fDOMendDate",by.y="endDate",all.x=F,all.y=F)

```

Let's take a look at a plot of fDOM versus discharge

```{r plot-fDOM-Q}
#Subset to high flow events
mergedDataSubset <- mergedData[mergedData$fDOMendDate > "2022-04-21" & mergedData$fDOMendDate < "2022-04-25",]

colfunc <- colorRampPalette(c("cyan","deeppink"))
#colfunc <- colorRampPalette(c("red","orange","yellow","green","blue","purple","deeppink"))

ggplot(data = mergedDataSubset, 
       aes(x = maxpostDischarge, y = fDOM)) +
  geom_point(color=colfunc(length(mergedDataSubset$fDOMendDate))[order(mergedDataSubset$fDOMendDate)]) + 
  ggtitle(paste0(siteID," fDOM vs. Q")) + 
  xlab("Q (L/s)") + 
  ylab("fDOM (QSU)")
```


Event 1 and event 2 appear to have similar shapes for both events for fDOM with event water being prominent in volume and concentrations of fDOM highest for soil water, then event water, and lowest for groundwater. 

![](C:/Users/kcawley/Documents/GitHub/WORKSHOP-AEMON-J-DSOS-2022/fDOM.png){#id .class width=60% height=60%}


### Challenge: Aggregate nitrate and continuous discharge
  
Using what you've learned above, can you modify the code to aggregate nitrate 
and continuous discharge?  Remember, nitrate is reported as a 15 minute average,
and there are a couple potential ways of dealing with this.

```{r aggregate-nitrate-csd, results='hide'}
# First let's remove any quality flagged measurements
NSW_15_minute<-NSW_15_minute[(NSW_15_minute$finalQF==0),]

# Round the nitrate time stamp to the nearest minute  
NSW_15_minute$endDateTime<-lubridate::round_date(NSW_15_minute$endDateTime,unit="1 minute")

# Merge the dataframes using the time stamps.  A simple approach is matching 
# every 15th discharge measurement with the corresponding nitrate measurement 
# and discarding the rest.
mergedData2<-merge(NSW_15_minute,csd_continuousDischarge,
                   by.x="endDateTime",by.y="endDate",
                   all.x=F,all.y=F)

mergedData2Subset <- mergedData2[mergedData2$endDate > "2022-04-21" & mergedData2$endDate < "2022-04-25",]

colfunc <- colorRampPalette(c("cyan","deeppink"))

ggplot(data = mergedData2Subset, 
       aes(x = maxpostDischarge, y = surfWaterNitrateMean)) +
  geom_point(color=colfunc(length(mergedData2Subset$endDate))[order(mergedData2Subset$endDate)]) + 
  ggtitle(paste0(siteID," NO3-N vs. Q (subset)")) + 
  xlab("Q (L/s)") + 
  ylab("NO3-N (uM)")

# # An alternative approach would be to first convert the csd dataframe into 
# # 15 minute averages.
# csd_15_minute<-csd_continuousDischarge[,c("endDate","maxpostDischarge")]
# csd_15_minute$endDate<-lubridate::round_date(csd_15_minute$endDate,unit="15 minute")
# csd_15_minute<-plyr::ddply(csd_15_minute,c("endDate"),summarise,maxpostDischarge=mean(maxpostDischarge))
# mergedData2b<-merge(NSW_15_minute,csd_15_minute,by.x="endDateTime",by.y="endDate",all.x=F,all.y=F)
# 
# ggplot(data = mergedData2b, 
#        aes(x = maxpostDischarge, y = surfWaterNitrateMean)) +
#   geom_point(color="green") + 
#   ggtitle(paste0(siteID," NO3-N vs. Q (avg)")) + 
#   xlab("Q (L/s)") + 
#   ylab("NO3-N (uM)")
```
Event 1 and event 2 have different shapes for nitrate concentration related to discharge. Event 1 shows a higher concentration in even water versus soil water, which event two shows more similar concentrations of nitrate in event and soil water.

![](C:/Users/kcawley/Documents/GitHub/WORKSHOP-AEMON-J-DSOS-2022/nitrate.png){#id .class width=90% height=90%}

Now that you have the basic tools and knowledge on how to read and wrangle NEON
AIS data, go have fun working on your scientific questions!

